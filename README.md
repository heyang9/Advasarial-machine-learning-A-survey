# Advasarial-machine-learning-A-survey
notes for ad-ml
1. Some conference paper

  Practical Evasion of a Learning-Based Classifier: A Case Study.Rndic et al, S&P,2014
  
  Intriguing properties of neural networks,C. Szegedy, et al, ICLR 2014.
  
  Explaining and Harnessing Adversarial Examples, I. Goodfellow et al., ICLR 2015
  
  Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images, A. Nguyen et al., CVPR 2015
  
  DeepFool: a simple and accurate method to fool deep neural networks, S. Moosavi-Dezfooli et al., CVPR 2016
  
  Automatically evading classifiers., Xu et al, NDSS 2016
  
  Distillation as a defense to adversarial perturbations against deep neural networks，N. Papernot et al, S&P 2016
  
  The limitations of deep learning in adversarial settings,N. Papernot et al, Euro S&P   
  2016
  
  Evasion and hardening of tree ensemble classifiers, Kantchelian et al, ICML2016.
  
  Hidden voice commands, N. Carlini et al, USENIX 2016.
  
  When a tree falls: Using diversity in ensemble classifiers to identify evasion in 
  malware detectors, C. Smutz et al, NDSS 2016.
  
  Delving into Transferable Adversarial Examples and Black-box Attacks Liu et al., ICLR   
  2017
  
  Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples, N. Papernot et al., Asia CCS 2017
  
  Tactics of Adversarial Attacks on Deep Reinforcement Learning Agents, Y. Lin et al, IJCAI 2017
  
  Adversarial Examples for Semantic Segmentation and Object Detection, C. Xie, ICCV 2017
  
  Adversarial Machine Learning At Scale, A. Kurakin et al., ICLR 2017
  
  Evading classifiers by morphing in the dark, Dang, SIGSAC 2017.
  
  Towards evaluating the robustness of neural networks,N. Carlini et al, S&P 2017.
  
  Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning, Jagielski, Matthew, et al. S&P 2018
  
  Trojaning Attack on Neural Networks, Yingqi Liu et al, NDSS 2018
  
  Spatially Transformed Adversarial Examples, Chaowei Xiao,ICLR 2018
  
  
2. Scenarios

  GENERAL
  
  Can Machine Learning Be Secure? [1]
  
   Adversarial Machine Learning[2]
   
   Pattern Recognition Systems Under Attack: Design Issues and Research Challenges [3]
   
  Adversarial Learning[4]
  
   Evasion attacks against machine learning at test time [5]
   
   A Framework for Quantitative Security Analysis of Machine Learning [6]
   
   Understanding the Risk Factors of Learning in Adversarial Environments [7]
   
   Security Evaluation of Support Vector Machines in Adversarial Environments [8]
   
  Image Classification （DNN）
  
   The Limitations of Deep Learning in Adversarial Settings[9]
   
  Online Learning
  
   Online Learning with Adversarial Delays [10]
   
   Online Learning under Delayed Feedback [11]
   
  Classification
  
   Adversarial Classification[12]
   
   Adversarial Pattern Classification Using Multiple Classifiers and Randomisation [13]
   
   Security evaluation of pattern classifiers under attack[14] 
   
  Spam Filter/ Malware detection
  
  Exploiting Machine Learning to Subvert Your Spam Filter[15] 
  
 Segmentation & Object Detection
 
 Defense
 
 Ref:
 [1] Barreno, Marco, et al. "Can machine learning be secure?." ACM Symposium on Information, Computer and Communications SecurityACM, 2006:16-25.
[2] Huang, Ling, et al. "Adversarial machine learning." ACM Workshop on Security and Artificial Intelligence 2011:43-58.
[3] BATTISTA BIGGIO, GIORGIO FUMERA, and FABIO ROLI. "PATTERN RECOGNITION SYSTEMS UNDER ATTACK: DESIGN ISSUES AND RESEARCH CHALLENGES." International Journal of Pattern Recognition & Artificial Intelligence 28.07(2014):1460002-.
[4] Lowd, Daniel, and C. Meek. "Adversarial learning." Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining ACM, 2005:641-647.
[5] Biggio, Battista, et al. "Evasion attacks against machine learning at test time." Th European Conference on Machine Learning and Knowledge Discovery in Databases Springer-Verlag, 2013:387-402.
[6] Laskov, Pavel, and M. Kloft. "A framework for quantitative security analysis of machine learning." ACM Workshop on Security and Artificial Intelligence ACM, 2009:1-4.
[7] Nelson, Blaine, B. Biggio, and P. Laskov. "Understanding the risk factors of learning in adversarial environments." ACM Workshop on Artificial Intelligence and Security ACM, 2011:87-92.
[8] Biggio, Battista, et al. "Security Evaluation of Support Vector Machines in Adversarial Environments." (2014):105-153.
[9] Papernot, Nicolas, et al. "The Limitations of Deep Learning in Adversarial Settings." IEEE European Symposium on Security and Privacy IEEE, 2016:372-387.
[10] Quanrud, Kent, and D. Khashabi. "Online learning with adversarial delays." International Conference on Neural Information Processing Systems MIT Press, 2015:1270-1278.
[11] Joulani, Pooria, A. György, and C. Szepesvári. "Online Learning under Delayed Feedback." Computer Science (2013):1453-1461.
[12] Dalvi, Nilesh, et al. "Adversarial classification." Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACM, 2004:99-108.
[13] Biggio, Battista, G. Fumera, and F. Roli. "Adversarial Pattern Classification Using Multiple Classifiers and Randomisation." Joint Iapr International Workshop on Structural, Syntactic, and Statistical Pattern Recognition Springer-Verlag, 2008:500-509.
[14] Biggio, Battista, G. Fumera, and F. Roli. "Security Evaluation of Pattern Classifiers under Attack." IEEE Transactions on Knowledge & Data Engineering 26.4(2014):984-996.
[15] Nelson, Blaine, et al. "Exploiting machine learning to subvert your spam filter." Usenix Workshop on Large-Scale Exploits and Emergent ThreatsUSENIX Association, 2008:7


